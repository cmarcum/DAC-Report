% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/download-dar-tables.R
\name{download.dar.tables}
\alias{download.dar.tables}
\title{Download DAR Tables}
\usage{
download.dar.tables(table.name, start.date, end.date, directory, wait.for = 5)
}
\arguments{
\item{table.name}{String, name of the table to query from}

\item{start.date}{String, the date to start scraping in mm/dd/yyyy format}

\item{end.date}{String, the date to end scraping in mm/dd/yyyy format}

\item{directory}{String, the path (file name included) in which the scraped data should be stored}

\item{wait.for}{integer, how many seconds to wait before sending another request to the server}
}
\value{
None
}
\description{
Download all daily data from the specified table within the time range as a csv file
This function scrapes the data available on the dbGaP Data Access and Use Report page
(https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/DataUseSummary.cgi)
}
\details{
table.name is one of table<1-9>, tablea1, tableb1, tablec1, tabled1.
Currently supports scraping table1,2,6,a1. Note that tablea1 is time invariant (same content regardless of start or end date)

The example will send 365 requests consecutively, each spaced by 5 seconds, to query table2
with the date intervals being 01/01/2020-01/02/2020, ..., 12/31/2020-01/01/2021
and store the result at './data/table2_2020.csv'
}
\examples{
\dontrun{
download.dar.tables('table2','01/01/2020','12/31/2020','./data/table2_2020.csv',5)
}
}
